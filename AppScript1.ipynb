{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c143323-11fa-46fd-84e7-5fd996c34c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (2.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-auth) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-auth) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2024.8.30)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (2.158.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-python-client) (2.37.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-python-client) (2.24.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.8.30)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\sonal gupta\\anaconda3\\lib\\site-packages (2.9.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-auth google-auth-oauthlib\n",
    "!pip install google-api-python-client\n",
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e347b1-46e1-4759-a159-cbeb7299e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, redirect, url_for\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.worksheet import Worksheet\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "from dotenv import load_dotenv\n",
    "import googleapiclient.discovery\n",
    "import os\n",
    "import openpyxl\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import pandas.io.sql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2117f25f-a839-4f07-8e9b-a3c64fe5342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  Sonal Gupta\n",
      "Enter the data name (Atom Banana or Get Fresh):  Atom Banana\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pick ticket file: C:\\Users\\Sonal Gupta\\Documents\\InventoryMove\\MO Pick Ticket - Atom Banana.csv\n",
      "Using inventory file: C:\\Users\\Sonal Gupta\\Documents\\InventoryMove\\InvQtys.csv\n",
      "Processed data will be saved to: C:\\Users\\Sonal Gupta\\Documents\\InventoryMove\\atom banana_master_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read and load pick ticket file and inventory file\n",
    "def load_data(pick_ticket_file, inventory_file):\n",
    "    pick_data = pd.read_csv(pick_ticket_file, encoding='latin1') \n",
    "    inventory_data = pd.read_csv(inventory_file, encoding='latin1')\n",
    "    return pick_data, inventory_data\n",
    "\n",
    "\n",
    "# function to compare row counts for pick ticket data\n",
    "def check_row_count(initial, current, step_name):\n",
    "    if initial < current:\n",
    "        raise ValueError(f\"Row count mismatch at {step_name}! Expected: {expected}, Found: {current}\")\n",
    "\n",
    "\n",
    "# data transofations for pick ticket\n",
    "def process_pick_data(pick_data, ab_or_gf): \n",
    "    \n",
    "    #pick ticket data frame\n",
    "    pick_data['staged_for_production'] = \"\"\n",
    "    pick_data['used_for_production'] = \"\"\n",
    "    pick_data['qty_returned_to_inventory'] = \"\"\n",
    "    pick_data = pick_data.dropna(axis=1, how='all')\n",
    "\n",
    "    pick_data = pick_data.rename(columns={\n",
    "        'PICKITEMPARTNUM': 'part_number',\n",
    "        'PICKITEMPARTDESC': 'description',\n",
    "        'PICKITEMQTYVUOM': 'pick_qty',\n",
    "        'VendorUOM': 'vendor_uom',\n",
    "        'CasesNeeded': 'cases_needed',\n",
    "        'CasesNeeded_v2': 'cases_rounded',\n",
    "        'MONUM': 'mo_number',\n",
    "        'PICKITEMUOM': 'pick_uom'\n",
    "    })\n",
    "\n",
    "    # drop uneeded columns\n",
    "    pick_data = pick_data.drop(['PICKDATESCHEDULED', 'PICKITEMSTATUS', 'PICKITEMQTY'], axis=1)\n",
    "    grouped_parts = pick_data.groupby('part_number')[['pick_qty', 'cases_needed', 'cases_rounded']].sum().reset_index() \n",
    "    #grouped_parts= pd.DataFrame(grouped_parts)\n",
    "\n",
    "    # drop uneeded columns\n",
    "    pick_data =  pick_data.drop(['pick_qty', 'cases_needed', 'cases_rounded'], axis=1)\n",
    "    pick_data = pick_data.merge(grouped_parts, on = 'part_number')\n",
    "    \n",
    "    # reindex columns to move the last column to the 5th position\n",
    "    cols = pick_data.columns.tolist()\n",
    "    last_col = cols.pop()\n",
    "    cols.insert(4, last_col)\n",
    "    pick_data = pick_data.reindex(columns=cols)\n",
    "\n",
    "    return pick_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data transofations for inventory sheet\n",
    "def process_inventory_data(inventory_data, ab_or_gf):\n",
    "\n",
    "    # filter inventory data\n",
    "    inventory_filtered = inventory_data[inventory_data['Location'].str.contains(ab_or_gf)]\n",
    "    inventory_sheet = inventory_filtered[['PartNumber', 'Location', 'Qty', 'UOM', 'Tracking-Lot Number', 'Tracking-Expiration Date']]\n",
    "\n",
    "    inventory_sheet = inventory_sheet.rename(columns={\n",
    "        'PartNumber': 'part_number',\n",
    "        'Location': 'beginning_location',\n",
    "        'Qty': 'on_hand',\n",
    "        'UOM': 'uom',\n",
    "        'Tracking-Lot Number': 'lot_number',\n",
    "        'Tracking-Expiration Date': 'expiration_date'\n",
    "    })\n",
    "\n",
    "    # new columns\n",
    "    inventory_sheet['end_location'] = f\"{ab_or_gf}_Meal Kit-Picking\" #shud work for ab, idk if gf has mkp\n",
    "    inventory_sheet['note'] = \"\"\n",
    "\n",
    "    return inventory_sheet\n",
    "\n",
    "\n",
    "# merges pick data and new inventory sheet\n",
    "def merge_data(pick_data, inventory_sheet):\n",
    "    master_data = pick_data.merge(inventory_sheet, on='part_number', how='inner')\n",
    "    return master_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# format, clean, and perform final transformations on the merged data.\n",
    "def format_and_clean_master_data(master_data):\n",
    "\n",
    "    # basic transformations\n",
    "    master_data['short_quantity'] = \"\"\n",
    "    master_data = master_data[['mo_number', 'beginning_location', 'lot_number', 'expiration_date', 'part_number', 'description', 'on_hand', 'uom', 'pick_qty',\n",
    "                               'vendor_uom', 'cases_needed', 'cases_rounded', 'staged_for_production',\n",
    "                               'used_for_production', 'qty_returned_to_inventory', 'short_quantity']]\n",
    "\n",
    "    # convert 'vendor_uom' column to numeric where applicable #changed\n",
    "    master_data.loc[master_data['vendor_uom'] == 'ea', 'vendor_uom'] = 1\n",
    "    master_data.loc[:, 'vendor_uom'] = master_data['vendor_uom'].copy().apply(drop_letters_after_number)\n",
    "\n",
    "\n",
    "    # exclude parts that are either in the Dandee location OR are already in a picking location\n",
    "    master_data = master_data[~master_data['beginning_location'].str.contains('Dandee|Picking|Prepared')]\n",
    "    master_data = master_data[~master_data['part_number'].str.startswith(('M-P', 'MK ', 'M-TA71103'))] #is this AB logic?\n",
    "\n",
    "    # replace NaN values in specific columns\n",
    "    columns_to_replace_nan = ['beginning_location', 'lot_number', 'on_hand', 'cases_needed', 'cases_rounded']\n",
    "    for column in columns_to_replace_nan:\n",
    "        master_data[column] = master_data[column].fillna(0)\n",
    "\n",
    "    # Set \"move\" column to FALSE (unchecked by default)\n",
    "    master_data['move'] = False\n",
    "    master_data['substitution'] = \"\"\n",
    "    \n",
    "    # resort specific values\n",
    "    master_data = master_data.sort_values(['part_number', 'lot_number', 'expiration_date'], ascending=True)\n",
    "    # master_data['move'] = \"\"\n",
    "    master_data['substitution'] = \"\"\n",
    "\n",
    "    # drop duplicated values\n",
    "    master_data = master_data.drop_duplicates()\n",
    "\n",
    "    # format numerical fields\n",
    "    for col in ['vendor_uom', 'pick_qty', 'cases_needed', 'cases_rounded', 'on_hand']:\n",
    "        master_data[col] = pd.to_numeric(master_data[col], errors='coerce')\n",
    "\n",
    "    # round numerical fields\n",
    "    master_data['pick_qty'] = round(master_data['pick_qty'], 2)\n",
    "    master_data['cases_on_hand'] = round(master_data['on_hand'] / master_data['vendor_uom'], 2)\n",
    "    master_data['part_count']= master_data.groupby('part_number').cumcount() + 1\n",
    "\n",
    "    return master_data\n",
    "\n",
    "# helper to clean up the vendor uom data column\n",
    "def drop_letters_after_number(input_string):\n",
    "    if isinstance(input_string, str):\n",
    "        match = re.search(r'\\d+(\\.\\d+)?', input_string)\n",
    "        return float(match.group()) if match else 0\n",
    "    return input_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function that's called in appscript to create checboxes for Move column\n",
    "def addCheckboxesAndSyncMove(sheet_name):\n",
    "    \n",
    "    # get the sheetId from the sheet name\n",
    "    spreadsheet = service.spreadsheets().get(spreadsheetId=SPREADSHEET_ID).execute()\n",
    "    sheets = spreadsheet.get('sheets', [])\n",
    "    \n",
    "    # find sheetId for the sheet_name\n",
    "    sheet_id = None\n",
    "    for sheet in sheets:\n",
    "        if sheet['properties']['title'] == sheet_name:\n",
    "            sheet_id = sheet['properties']['sheetId']\n",
    "            print(f\"Found sheetId for '{sheet_name}': {sheet_id}\")\n",
    "            break\n",
    "    \n",
    "    if sheet_id is None:\n",
    "        print(f\"Error: Sheet with name '{sheet_name}' not found.\")\n",
    "        return\n",
    "\n",
    "    # set the column number for 'Move' and 'Used for Production'\n",
    "    header_row = 1\n",
    "    move_col = None\n",
    "    used_for_prod_col = None\n",
    "\n",
    "    # get the sheet data to find the columns for 'Move' and 'Used for Production'\n",
    "    range_to_check = f\"{sheet_name}!A1:Z1\"\n",
    "    sheet_values = service.spreadsheets().values().get(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=range_to_check\n",
    "    ).execute()\n",
    "\n",
    "    headers = sheet_values.get('values', [])[0]\n",
    "    \n",
    "    try:\n",
    "        move_col = headers.index('Move') + 1  # 1-based index for columns\n",
    "        used_for_prod_col = headers.index('Used for Production') + 1\n",
    "    except ValueError:\n",
    "        print(f\"Error: Columns 'Move' or 'Used for Production' not found in the header row.\")\n",
    "        return\n",
    "\n",
    "    # apply checkboxes to the 'Move' column\n",
    "    requests = []\n",
    "\n",
    "    # insert checkboxes in the 'Move' column\n",
    "    requests.append({\n",
    "        'updateCells': {\n",
    "            'range': {\n",
    "                'sheetId': sheet_id,\n",
    "                'startRowIndex': 1,  # start at row 2 to avoid the header\n",
    "                'endRowIndex': 100,  \n",
    "                'startColumnIndex': move_col - 1,  # converting to 0-based index\n",
    "                'endColumnIndex': move_col\n",
    "            },\n",
    "            'rows': [\n",
    "                {\n",
    "                    'values': [\n",
    "                        {\n",
    "                            'userEnteredValue': {'boolValue': False},  \n",
    "                            'dataValidation': {\n",
    "                                'condition': {\n",
    "                                    'type': 'BOOLEAN',\n",
    "                                    'values': [\n",
    "                                        {'userEnteredValue': 'TRUE'},\n",
    "                                        {'userEnteredValue': 'FALSE'}\n",
    "                                    ]\n",
    "                                },\n",
    "                                'showCustomUi': True\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                } for _ in range(99)  \n",
    "            ],\n",
    "            'fields': 'userEnteredValue,dataValidation'\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # this could be uneeded code\n",
    "    # fetch data to update checkboxes in the 'Move' column based on Used for Production\n",
    "    data = service.spreadsheets().values().get(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=f\"{sheet_name}!A2:Z100\"\n",
    "    ).execute()\n",
    "\n",
    "    rows = data.get('values', [])\n",
    "    \n",
    "    # update checkboxes based on Used for Production\n",
    "    for i, row in enumerate(rows):\n",
    "        used_for_prod_value = row[used_for_prod_col - 1] if len(row) > used_for_prod_col - 1 else None\n",
    "        move_checkbox_value = True if used_for_prod_value else False\n",
    "        \n",
    "        requests.append({\n",
    "            'updateCells': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'startRowIndex': i + 1,  # Skipping header\n",
    "                    'endRowIndex': i + 2,\n",
    "                    'startColumnIndex': move_col - 1,\n",
    "                    'endColumnIndex': move_col\n",
    "                },\n",
    "                'rows': [{\n",
    "                    'values': [{\n",
    "                        'userEnteredValue': {'boolValue': move_checkbox_value}\n",
    "                    }]\n",
    "                }],\n",
    "                'fields': 'userEnteredValue'\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # sendbatch update request\n",
    "    if requests:\n",
    "        try:\n",
    "            response = service.spreadsheets().batchUpdate(\n",
    "                spreadsheetId=SPREADSHEET_ID,\n",
    "                body={'requests': requests}\n",
    "            ).execute()\n",
    "            print(f\"Successfully applied batch update for sheet {sheet_name}\")\n",
    "        except HttpError as err:\n",
    "            print(f\"HttpError: {err}\")\n",
    "            return\n",
    "\n",
    "    print(f\"Checkboxes synced in the 'Move' column for {sheet_name}\")\n",
    "\n",
    "\n",
    "\n",
    "# calculating totals\n",
    "def calculate_totals(master_data):\n",
    "\n",
    "    # calculate total on-hand sum\n",
    "    total_on_hand = master_data.groupby('part_number')['on_hand'].sum().reset_index()\n",
    "    total_on_hand.columns = ['part_number', 'total_on_hand']\n",
    "\n",
    "    # total on hand cases\n",
    "    total_cases_on_hand = master_data.groupby('part_number')['cases_on_hand'].sum().reset_index()\n",
    "    total_cases_on_hand.columns = ['part_number', 'total_cases_on_hand']\n",
    "\n",
    "    # merge total_on_hand DF into master_data\n",
    "    master_data = master_data.merge(total_on_hand, on='part_number', how='left')\n",
    "    master_data = master_data.merge(total_cases_on_hand, on='part_number', how='left')\n",
    "\n",
    "    return master_data\n",
    "\n",
    "# final dataframe; full proccessing of the entire data\n",
    "def process_data(pick_ticket_file, inventory_file, ab_or_gf):\n",
    "\n",
    "    # load data\n",
    "    pick_data, inventory_data = load_data(pick_ticket_file, inventory_file)\n",
    "    print(inventory_data.columns)\n",
    "\n",
    "\n",
    "    # row calculations for cheecks\n",
    "    initial_pick_rows = len(pick_data)\n",
    "    initial_inventory_rows = len(inventory_data)\n",
    "    print(f\"Initial pick rows: {initial_pick_rows}\")\n",
    "    print(f\"Initial inventory rows: {initial_inventory_rows}\")\n",
    "\n",
    "    # process pick and inventory data\n",
    "    pick_data = process_pick_data(pick_data, ab_or_gf)\n",
    "    inventory_sheet = process_inventory_data(inventory_data, ab_or_gf)\n",
    "    print(inventory_data.columns) # added to verify the column names on the inventory sheet\n",
    "\n",
    "    # check row counts after processing pick ticket data\n",
    "    check_row_count(initial_pick_rows, len(pick_data), f\"{ab_or_gf} Pick Processing\")\n",
    "    check_row_count(initial_inventory_rows, len(inventory_sheet), f\"{ab_or_gf} Inventory Processing\")\n",
    "\n",
    "    # merge the pick and inventory data\n",
    "    master_data = merge_data(pick_data, inventory_sheet)\n",
    "\n",
    "    # format and clean the master data\n",
    "    master_data = format_and_clean_master_data(master_data)\n",
    "\n",
    "    # calculate totals\n",
    "    master_data = calculate_totals(master_data)\n",
    "\n",
    "    # final dataframe formatting\n",
    "    master_data = master_data[['mo_number', 'beginning_location', 'lot_number', 'expiration_date', 'part_number', 'description',\n",
    "                               'pick_qty', 'uom', 'vendor_uom', 'cases_needed', 'cases_rounded', 'staged_for_production',\n",
    "                               'used_for_production', 'qty_returned_to_inventory', 'move', 'substitution', 'on_hand',\n",
    "                               'cases_on_hand', 'total_on_hand', 'total_cases_on_hand']]\n",
    "\n",
    "    return master_data\n",
    "\n",
    "#INITIAL SET UP FOR SCRIPT\n",
    "# (1) Open File Explorer on you laptop\n",
    "# (2) In the navigation of your left, click on Documents\n",
    "# (3) In the navigation at the top select \"New\" and create a New Folder titled: \"InventoryMove\"\n",
    "# (4) Download the needed files, and move them to the InventoryMove Folder\n",
    "# (5) Renaming\n",
    "    # (1) Rename the Atom Banana Pick Ticket File to: MO Pick Ticket - Atom Banana\n",
    "    # (2) Rename the Get Fresh Pick Ticket File to: MO Pick Ticket - Get Fresh\n",
    "    # (3) Rename the Inventory Quantities file to: InvQtys\n",
    "    # (4) Rename the Master Data Spreadsheet file to: ab_master_data\n",
    "    # (5) Make sure you have the \"tokens.json\" and \"credentials.json\" in the InventoryMove Folder\n",
    "\n",
    "# RUNNING THE SCRIPT EACH TIME\n",
    "# (6) Run the script. It will ask for 2 inputs.\n",
    "    # (1) Enter your name (this should be the name you use for your laptop (ex. John Smith)\n",
    "    # (2) Enter the data you are looking for (ex. Atom Banana or Get Fresh)\n",
    "# (7) Everything should now populate on the MK Google Sheet\n",
    "\n",
    "        \n",
    "\n",
    "# Ask user for their name\n",
    "username = input(\"Enter your name: \").strip()\n",
    "\n",
    "# Define base directory where files must be stored\n",
    "base_directory = f\"C:\\\\Users\\\\{username}\\\\Documents\\\\InventoryMove\"\n",
    "\n",
    "# Ensure the base directory exists\n",
    "os.makedirs(base_directory, exist_ok=True)\n",
    "\n",
    "# Ask user for Atom Banana or Get Fresh\n",
    "ab_or_gf = input(\"Enter the data name (Atom Banana or Get Fresh): \").strip().title()\n",
    "\n",
    "# Determine the correct pick ticket file name\n",
    "if ab_or_gf == \"Atom Banana\":\n",
    "    pick_file_name = \"MO Pick Ticket - Atom Banana.csv\"\n",
    "elif ab_or_gf == \"Get Fresh\":\n",
    "    pick_file_name = \"MO Pick Ticket - Get Fresh.csv\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid input! Please enter 'Atom Banana' or 'Get Fresh'.\")\n",
    "\n",
    "# Construct full file paths\n",
    "pick_file = os.path.join(base_directory, pick_file_name)\n",
    "inventory_file = os.path.join(base_directory, \"InvQtys.csv\")\n",
    "output_path = os.path.join(base_directory, f\"{ab_or_gf.lower()}_master_data.xlsx\")\n",
    "\n",
    "print(f\"Using pick ticket file: {pick_file}\")\n",
    "print(f\"Using inventory file: {inventory_file}\")\n",
    "print(f\"Processed data will be saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254da536-c986-49f3-a01c-9ea793ef953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mo_number: MK 2025 02/10/2025\n",
      "{'spreadsheetId': '1NuVTf2n7DGi3DhJLGz6E_Tt2GJiGt0BxqoldptK4CK8', 'replies': [{'addSheet': {'properties': {'sheetId': 203523933, 'title': 'MK 2025 02/10/2025', 'index': 4, 'sheetType': 'GRID', 'gridProperties': {'rowCount': 1000, 'columnCount': 26}}}}]}\n",
      "Found sheetId for 'MK 2025 02/10/2025': 203523933\n",
      "Successfully applied batch update for sheet MK 2025 02/10/2025\n",
      "Checkboxes synced in the 'Move' column for MK 2025 02/10/2025\n",
      "Data copied for sheet MK 2025 02/10/2025\n",
      "After copying data for all sheets\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "#this is how the script accesses the google sheet \n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "SPREADSHEET_ID = '1NuVTf2n7DGi3DhJLGz6E_Tt2GJiGt0BxqoldptK4CK8'\n",
    "\n",
    "creds = None\n",
    "token_directory = base_directory\n",
    "token_filename = \"token.json\"\n",
    "token_path = os.path.join(token_directory, token_filename)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(token_directory, exist_ok=True)\n",
    "\n",
    "if os.path.exists(token_path):\n",
    "    creds = Credentials.from_authorized_user_file(token_path)\n",
    "\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        credentials_path = os.path.join(base_directory, \"credentials.json\")\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "\n",
    "    with open(token_path, 'w') as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "service = googleapiclient.discovery.build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "def create_new_sheet(sheet_title):\n",
    "    request = {\n",
    "        'requests': [\n",
    "            {\n",
    "                'addSheet': {\n",
    "                    'properties': {\n",
    "                        'title': sheet_title\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = service.spreadsheets().batchUpdate(\n",
    "    spreadsheetId=SPREADSHEET_ID,\n",
    "    body=request\n",
    "    ).execute()\n",
    "\n",
    "    new_sheet_id = response['replies'][0]['addSheet']['properties']['sheetId']\n",
    "\n",
    "    # Set the column widths for the new mk tab\n",
    "    column_widths = [115,214,83,389, 100, 100,127,100,136,136,160,160,100,100,100,100,100,100,100,110, 110, 110]  # Adjust widths as needed\n",
    "\n",
    "    for i, width in enumerate(column_widths):\n",
    "        service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            body={'requests': [{'updateDimensionProperties': {'range': {'sheetId': new_sheet_id, 'dimension': 'COLUMNS', 'startIndex': i, 'endIndex': i + 1}, 'properties': {'pixelSize': width}, 'fields': 'pixelSize'}}]}\n",
    "        ).execute()\n",
    "\n",
    "    print(response)\n",
    "\n",
    "\n",
    "def sheet_exists(sheet_name):\n",
    "    try:\n",
    "        existing_sheet = service.spreadsheets().get(spreadsheetId=SPREADSHEET_ID, includeGridData=False).execute()\n",
    "        sheets = existing_sheet['sheets']\n",
    "        sheet_titles = [sheet['properties']['title'] for sheet in sheets]\n",
    "        \n",
    "        return sheet_name in sheet_titles\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking sheet existence: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def copy_data_to_mk_tab(monum_value):\n",
    "    source_range = f'master_data!A:Z'  # Adjust the range to cover all columns\n",
    "    destination_range = f'{monum_value}!A2:Z'  # Adjust the destination range\n",
    "\n",
    "    # Get all values from the source range\n",
    "    source_values = service.spreadsheets().values().get(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=source_range\n",
    "    ).execute()['values']\n",
    "\n",
    "    if not source_values:\n",
    "        print(f\"No data found in source range: {source_range}\")\n",
    "        return\n",
    "\n",
    "    source_values = [[col for i, col in enumerate(row) if i not in [2, 3]] for row in source_values]\n",
    "\n",
    "    expected_columns = 22\n",
    "    for row in source_values:\n",
    "         while len(row) < expected_columns:  # pad missing columns with None\n",
    "             row.append(None)\n",
    "         if len(row) > expected_columns:  # trim excess columns\n",
    "             row = row[:expected_columns]\n",
    "\n",
    "    # Create a DataFrame from the source data\n",
    "    df_master_data = pd.DataFrame(source_values[1:], columns=source_values[0])\n",
    "\n",
    "    # Filter based on both 'part_number' and 'MONUM'\n",
    "    df_filtered = df_master_data[(df_master_data['part_number'].notnull()) & (df_master_data['mo_number'] == monum_value)]\n",
    "\n",
    "    # Drop duplicates based on 'part_number' column\n",
    "    df_filtered = df_filtered.drop_duplicates('part_number', keep='first').reset_index(drop=True)\n",
    "    df_filtered = df_filtered.iloc[:, [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
    "\n",
    "    # Convert the filtered DataFrame back to a list of lists\n",
    "    filtered_rows = df_filtered.values.tolist()\n",
    "\n",
    "    # Skip copying if there are no matching rows\n",
    "    if not filtered_rows:\n",
    "        print(f'No matching data for sheet {monum_value}')\n",
    "        return\n",
    "\n",
    "    # Copy the filtered rows to the destination range\n",
    "    service.spreadsheets().values().update(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=destination_range,\n",
    "        valueInputOption='RAW',\n",
    "        body={'values': filtered_rows}\n",
    "    ).execute()\n",
    "\n",
    "    # Add checkboxes to 'Move' and sync based on 'Used for Production'\n",
    "    addCheckboxesAndSyncMove(monum_value)\n",
    "\n",
    "#creating the headers in the master data tab\n",
    "def sheet_has_headers(sheet_name='master_data'):\n",
    "    sheet = service.spreadsheets().values().get(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=f'{sheet_name}!A1:Z1',  # Assuming your data starts from column A\n",
    "    ).execute()\n",
    "\n",
    "    values = sheet.get('values', [])\n",
    "\n",
    "    return len(values) > 0\n",
    "\n",
    "# uploading the master excel sheet to the google sheet\n",
    "def upload_xlsx_to_sheets():\n",
    "    xlsx_file_path = f\"C:\\\\Users\\\\{username}\\\\Documents\\\\InventoryMove\\\\ab_master_data.xlsx\"\n",
    "    workbook = openpyxl.load_workbook(xlsx_file_path)\n",
    "    sheet = workbook.active\n",
    "\n",
    "    values = []\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        values.append(list(row))\n",
    "\n",
    "    header_row = values[0]\n",
    "    data_rows = values[1:]\n",
    "\n",
    "    # Add headers to the 'master_data' sheet if it doesn't have headers already\n",
    "    if not sheet_has_headers():\n",
    "        headers_range = 'master_data!A1:Z1'  # Assuming your data starts from column A\n",
    "        service.spreadsheets().values().update(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            range=headers_range,\n",
    "            valueInputOption='RAW',\n",
    "            body={'values': [header_row]}\n",
    "        ).execute()\n",
    "\n",
    "    # Append the existing data to the 'master_data' sheet\n",
    "    service.spreadsheets().values().append(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range='master_data!A1',\n",
    "        valueInputOption='RAW',\n",
    "        body={'values': data_rows},\n",
    "        insertDataOption='INSERT_ROWS'\n",
    "    ).execute()\n",
    "\n",
    "    # Create a DataFrame from the existing data\n",
    "    df = pd.read_excel(xlsx_file_path)\n",
    "\n",
    "    unique_monum_values = df['mo_number'].unique()\n",
    "\n",
    "    for monum_value in unique_monum_values:\n",
    "        print(f\"Processing mo_number: {monum_value}\")\n",
    "\n",
    "        if not sheet_exists(monum_value):\n",
    "            create_new_sheet(monum_value)\n",
    "\n",
    "            # Get the new sheet range\n",
    "            new_sheet_range = f'{monum_value}!A1:Z1' \n",
    "\n",
    "            # Add headers to the new sheet if it doesn't have headers already\n",
    "            if not sheet_has_headers(monum_value):\n",
    "                headers = [\n",
    "                    'MONUM', 'Beginning Location', 'Part Number', 'Part Description', 'Pick Qty', 'UOM', 'Vendor UOM', \n",
    "                    'Cases Needed', 'Total Cases Rounded','Staged for Production', 'Used for Production', \n",
    "                    'Qty Returned to Inventory', 'Move','Part Count', 'short_quantity', 'Substitution',\n",
    "                    'On Hand Qty', 'On Hand Cases', 'Total On Hand Qty', 'Total Cases'\n",
    "                ]\n",
    "\n",
    "\n",
    "                # Convert the headers to values\n",
    "                headers_values = [headers]\n",
    "\n",
    "                # Append the headers to the new sheet\n",
    "                service.spreadsheets().values().update(\n",
    "                    spreadsheetId=SPREADSHEET_ID,\n",
    "                    range=new_sheet_range,\n",
    "                    valueInputOption='RAW',\n",
    "                    body={'values': headers_values}\n",
    "                ).execute()\n",
    "\n",
    "            copy_data_to_mk_tab(monum_value) \n",
    "            print(f'Data copied for sheet {monum_value}')\n",
    "            \n",
    "\n",
    "    print(\"After copying data for all sheets\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    upload_xlsx_to_sheets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b7be5-f3f3-4859-8bd9-fe3f567f9730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
